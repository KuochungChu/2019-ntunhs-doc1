{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0c73c3f0544e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-mllib-lr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-mllib-lr>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"pyspark-mllib-lr\").getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binary classification in pySpark\n",
    "* 共有九個變數 (八個自變數，一個應變數)\n",
    "* Pregnancies: 懷孕次數\n",
    "* Glucose: 葡萄糖\n",
    "* BloodPressure:血壓\n",
    "* SkinThickness: 皮膚厚度 \n",
    "* Insulin: 胰島素\n",
    "* BMI: 來衡量肥胖程度，其計算公式是以體重（公斤）除以身高（公尺）的平方\n",
    "* DiabetesPedigreeFunction: 糖尿病家族函數\n",
    "* Age: 年紀 (歲)\n",
    "* Outcome: 類別結果 (0或1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome\r",
      "\r\n",
      "6,148,72,35,0,33.6,0.627,50,1\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#資料檢查\n",
    "cat /home/jovyan/dataset/diabetes.csv | head -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'Insulin',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age',\n",
       " 'Outcome']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#讀取資料(資料格式為dataframe)\n",
    "raw_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(r\"/home/jovyan/dataset/diabetes.csv\")\n",
    "raw_data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 敘述型統計資訊\n",
    "* 一般來說，應該還有一個步驟是資料預處理(preprocessing)，請參考講義參考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|Summary|       Pregnancies|          Glucose|     BloodPressure|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|               768|              768|               768|\n",
      "|   mean|3.8450520833333335|     120.89453125|       69.10546875|\n",
      "| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|\n",
      "|    min|                 0|                0|                 0|\n",
      "|    max|                17|              199|               122|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#關注一下敘述型統計資訊\n",
    "raw_data.describe().select(\"Summary\",\"Pregnancies\",\"Glucose\",\"BloodPressure\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|Summary|     SkinThickness|           Insulin|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               768|               768|\n",
      "|   mean|20.536458333333332| 79.79947916666667|\n",
      "| stddev|15.952217567727642|115.24400235133803|\n",
      "|    min|                 0|                 0|\n",
      "|    max|                99|               846|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"SkinThickness\",\"Insulin\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------------+------------------+\n",
      "|Summary|               BMI|DiabetesPedigreeFunction|               Age|\n",
      "+-------+------------------+------------------------+------------------+\n",
      "|  count|               768|                     768|               768|\n",
      "|   mean|31.992578124999977|      0.4718763020833327|33.240885416666664|\n",
      "| stddev| 7.884160320375441|       0.331328595012775|11.760231540678689|\n",
      "|    min|               0.0|                   0.078|                21|\n",
      "|    max|              67.1|                    2.42|                81|\n",
      "+-------+------------------+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 發現有一些欄位的最小值為 0 \n",
    "## 除了 Pregnancies 之外，其他的欄位最小值為0，這是有問題的\n",
    "* Pregnancies\n",
    "* Glucose\n",
    "* BloodPressure\n",
    "* SkinThickness\n",
    "* Insulin\n",
    "* BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將以下這些欄位為0的部分改為 NaN\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"Glucose\",when(raw_data.Glucose==0, np.nan).otherwise(raw_data.Glucose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"BloodPressure\",when(raw_data.BloodPressure==0, np.nan).otherwise(raw_data.BloodPressure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"SkinThickness\",when(raw_data.SkinThickness==0, np.nan).otherwise(raw_data.SkinThickness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"Insulin\",when(raw_data.Insulin==0, np.nan).otherwise(raw_data.Insulin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"BMI\",when(raw_data.BMI==0, np.nan).otherwise(raw_data.BMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-------------+----+-------+\n",
      "|Summary|Glucose|BloodPressure|SkinThickness| BMI|Insulin|\n",
      "+-------+-------+-------------+-------------+----+-------+\n",
      "|  count|    768|          768|          768| 768|    768|\n",
      "|   mean|    NaN|          NaN|          NaN| NaN|    NaN|\n",
      "| stddev|    NaN|          NaN|          NaN| NaN|    NaN|\n",
      "|    min|   44.0|         24.0|          7.0|18.2|   14.0|\n",
      "|    max|    NaN|          NaN|          NaN| NaN|    NaN|\n",
      "+-------+-------+-------------+-------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#當為NaN時候，是無法計算的\n",
    "raw_data.describe().select(\"Summary\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填補遺缺值\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#因為變成NaN了，將當於有遺缺值了\n",
    "#預設有三種方式填充遺缺值 (平均值(mean)、中位數(median))\n",
    "imputer=Imputer(inputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"],\n",
    "                outputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"]).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|  148.0|         72.0|         35.0|  125.0|33.6|                   0.627| 50|      1|\n",
      "|          1|   85.0|         66.0|         29.0|  125.0|26.6|                   0.351| 31|      0|\n",
      "|          8|  183.0|         64.0|         29.0|  125.0|23.3|                   0.672| 32|      1|\n",
      "|          1|   89.0|         66.0|         23.0|   94.0|28.1|                   0.167| 21|      0|\n",
      "|          0|  137.0|         40.0|         35.0|  168.0|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#填入遺缺值\n",
    "model=imputer.fit(raw_data)\n",
    "raw_data=model.transform(raw_data)\n",
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將應變數移除\n",
    "cols=raw_data.columns\n",
    "cols.remove(\"Outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark mllib有規定，X變數都必須為一維向量\n",
    "#將資料轉成一維向量\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|features                                    |\n",
      "+--------------------------------------------+\n",
      "|[6.0,148.0,72.0,35.0,125.0,33.6,0.627,50.0] |\n",
      "|[1.0,85.0,66.0,29.0,125.0,26.6,0.351,31.0]  |\n",
      "|[8.0,183.0,64.0,29.0,125.0,23.3,0.672,32.0] |\n",
      "|[1.0,89.0,66.0,23.0,94.0,28.1,0.167,21.0]   |\n",
      "|[0.0,137.0,40.0,35.0,168.0,43.1,2.288,33.0] |\n",
      "|[5.0,116.0,74.0,29.0,125.0,25.6,0.201,30.0] |\n",
      "|[3.0,78.0,50.0,32.0,88.0,31.0,0.248,26.0]   |\n",
      "|[10.0,115.0,72.0,29.0,125.0,35.3,0.134,29.0]|\n",
      "|[2.0,197.0,70.0,45.0,543.0,30.5,0.158,53.0] |\n",
      "|[8.0,125.0,96.0,29.0,125.0,32.3,0.232,54.0] |\n",
      "|[4.0,110.0,92.0,29.0,125.0,37.6,0.191,30.0] |\n",
      "|[10.0,168.0,74.0,29.0,125.0,38.0,0.537,34.0]|\n",
      "|[10.0,139.0,80.0,29.0,125.0,27.1,1.441,57.0]|\n",
      "|[1.0,189.0,60.0,23.0,846.0,30.1,0.398,59.0] |\n",
      "|[5.0,166.0,72.0,19.0,175.0,25.8,0.587,51.0] |\n",
      "|[7.0,100.0,72.0,29.0,125.0,30.0,0.484,32.0] |\n",
      "|[0.0,118.0,84.0,47.0,230.0,45.8,0.551,31.0] |\n",
      "|[7.0,107.0,74.0,29.0,125.0,29.6,0.254,31.0] |\n",
      "|[1.0,103.0,30.0,38.0,83.0,43.3,0.183,33.0]  |\n",
      "|[1.0,115.0,70.0,30.0,96.0,34.6,0.529,32.0]  |\n",
      "+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#檢查看看，是否是一維向量\n",
    "raw_data=assembler.transform(raw_data)\n",
    "raw_data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料預處理，通常我們會用 StandardScaler 處理\n",
    "* StandardScaler=(原始數值-平均數)/標準差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#進行StandardScaler處理\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_data=standardscaler.fit(raw_data).transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     Scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[6.0,148.0,72.0,3...|[1.78063837321943...|\n",
      "|[1.0,85.0,66.0,29...|[0.29677306220323...|\n",
      "|[8.0,183.0,64.0,2...|[2.37418449762590...|\n",
      "|[1.0,89.0,66.0,23...|[0.29677306220323...|\n",
      "|[0.0,137.0,40.0,3...|[0.0,4.5009104914...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 建立一個新欄位，將既有的 features 欄位 Standard scalarizer 後，產生一個新的欄位 Scaled_features\n",
    "raw_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立訓練集(80%)、測試集(20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = raw_data.randomSplit([0.8,0.2], seed=123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 檢查是否有imbalance的問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "為1: 221\n",
      "為0: 402.0\n",
      "為1的百分比: 35.47351524879615\n"
     ]
    }
   ],
   "source": [
    "#檢查是否有imbalance的問題\n",
    "dataset_size=float(train.select(\"Outcome\").count())\n",
    "numPositives=train.select(\"Outcome\").where(\"Outcome==1\").count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('為1: {}'.format(numPositives))\n",
    "print('為0: {}'.format(numNegatives))\n",
    "print('為1的百分比: {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalancingRatio=0.6452648475120385\n"
     ]
    }
   ],
   "source": [
    "# 的確有 imbalance的問題，為1的比例為35.47%，因此我們可以將為1的資料給予一個較大的權重\n",
    "# BalancingRatio=numNegatives/dataset_size\n",
    "\n",
    "BalancingRation=numNegatives/dataset_size\n",
    "print('BalancingRatio={}'.format(BalancingRation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|Outcome|      classWeights|\n",
      "+-------+------------------+\n",
      "|      0|0.3547351524879615|\n",
      "|      0|0.3547351524879615|\n",
      "|      0|0.3547351524879615|\n",
      "|      0|0.3547351524879615|\n",
      "|      0|0.3547351524879615|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#為0: 我們放入classWeights，內容為1-BalancingRatio\n",
    "train=train.withColumn(\"classWeights\",when(train.Outcome==1, BalancingRation).otherwise(1-BalancingRation))\n",
    "train.select(\"Outcome\", \"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|Outcome|      classWeights|\n",
      "+-------+------------------+\n",
      "|      1|0.6452648475120385|\n",
      "|      1|0.6452648475120385|\n",
      "|      1|0.6452648475120385|\n",
      "|      1|0.6452648475120385|\n",
      "|      1|0.6452648475120385|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#為1: 我們放入classWeights，內容為BalancingRatio\n",
    "train.filter(train['Outcome'] == 1).select(\"Outcome\", \"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection (採用 ChiSqSelector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark MLlib 具有三種方法找特徵值 (VectorSlicer/RFormula/ChiSqSelector)\n",
    "#Chi-Squared feature selection: 適用於目標值為類別型變數\n",
    "\n",
    "from pyspark.ml.feature import ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fpr設定為0.05\n",
    "#FPR=FP(錯誤分到正樣本數量)/(FP + TN)\n",
    "#TPR=TP/(TP+FN)\n",
    "css=ChiSqSelector(featuresCol='Scaled_features',outputCol='Aspect',labelCol='Outcome',fpr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=css.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=css.fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Aspect                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.0,2.2011752038500245,6.282735462564321,3.2987454102033076,1.4470429786115504,6.588921448662343,0.5855214518762558,3.911487613223074]|\n",
      "|[0.0,2.7596823451254036,5.290724600054165,2.502496518085268,0.7640386927068986,5.207138804903132,1.6448927385183476,1.785679127775751] |\n",
      "|[0.0,2.989655873885854,5.62139488755755,3.639994935396753,2.431032204067405,5.803487103788687,1.1499158410559458,2.125808485447323]    |\n",
      "|[0.0,3.1210693188918257,6.613405750067706,5.118742877901684,1.065023632258101,5.3089543681274955,0.995990098552394,2.210840824865216]  |\n",
      "|[0.0,3.252482763897797,5.952065175060935,3.2987454102033076,1.4470429786115504,3.6362701151558188,0.7635924088901687,1.870711467193644]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#提供模型變數挑選參考 (Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age)\n",
    "test.select(\"Aspect\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立羅吉斯回歸分類演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"Aspect\", weightCol=\"classWeights\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#進行預測\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       1.0|\n",
      "|      1|       0.0|\n",
      "|      1|       0.0|\n",
      "|      0|       0.0|\n",
      "|      1|       1.0|\n",
      "|      1|       1.0|\n",
      "|      0|       0.0|\n",
      "|      0|       1.0|\n",
      "|      1|       1.0|\n",
      "|      0|       1.0|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#預測結果與實際值比較\n",
    "predict_test.select(\"Outcome\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型評估\n",
    "*預設用ROC評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#原始預測值rawPredictionCol\n",
    "#因為等等要做ROC，所以需要做這個\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"Outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+\n",
      "|Outcome|       rawPrediction|prediction|         probability|\n",
      "+-------+--------------------+----------+--------------------+\n",
      "|      0|[1.93251831726415...|       0.0|[0.87352789759305...|\n",
      "|      0|[1.58456515697427...|       0.0|[0.82985008420462...|\n",
      "|      0|[1.49187237243758...|       0.0|[0.81635913912982...|\n",
      "|      0|[1.88417237871225...|       0.0|[0.86808963982654...|\n",
      "|      0|[2.87706309674848...|       0.0|[0.94670086661773...|\n",
      "+-------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.select(\"Outcome\",\"rawPrediction\",\"prediction\",\"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for training set is 0.8351005155219385\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for training set is {}\".format(evaluator.evaluate(predict_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for testing set is 0.8712548849326966\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for testing set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超參數\n",
    "* 模型優化(model tuning)\n",
    "\n",
    "* aggregationDepth: 調整層次深度，該值越大，表示能更快處理資料(>=2的整數，預設為2)\n",
    "* elasticNetParam: 正規化調整計算參數(0~1之間，預設為：0)\n",
    "* family: 採用分類方式 (二元分類)\n",
    "* featuresCol: 輸入訓練集、測試集之變數向量(預設為features)\n",
    "* fitIntercept: 是否使用帶截距的回歸。(預設為true)\n",
    "* labelCol: 目標欄位(預設為label)\n",
    "* maxIter: 迭代算法最大迭代次數，搭配tol值一起設定的結束條件(預設為100)\n",
    "* tol: 收斂容忍係數(convergence tolerance)，算法每次迭代後的比較阈值以確定算法是否結束，值越小，执行的迭代次數就越多(預設值1.0E-6)\n",
    "* predictionCol: 輸出結果資料中最終判別類別名稱(預設值為prediction)\n",
    "* probabilityCol: 輸出結果資料之Softmax函數機率值的名稱(預設為probability)\n",
    "* rawPredictionCol: 輸出結果資料之回歸應變數名稱(預設為rawPrediction)\n",
    "* regParam: 正規化懲罰程度參數(預設為0) \n",
    "* standardization: 是否在回歸前對自變數進行標準化處理(預設為True)\n",
    "* threshold: 二元分類演算法之中根據該值將計算得到之邏輯函數機率值對應於分類類別中(預設為5)\n",
    "* thresholds: 多元分類演算法中根據該組數值將計算後得到的Softmax函數機率對應於多元分類中(無預設值)\n",
    "* weightCol: 輸入訓練集權重的名稱，如果輸入的訓練集包含權重，則採用帶權重的回歸模型，如果不包含，相當於全重是1.0 (無預設值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#進行超參數(Grid)，通常都會搭配(CV)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#設計一個超參數grid，給定參數的值域(list)\n",
    "paramGrid=ParamGridBuilder() \\\n",
    ".addGrid(lr.aggregationDepth,[2,5,10]) \\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    ".addGrid(lr.fitIntercept,[False, True]) \\\n",
    ".addGrid(lr.maxIter,[10,100,1000]) \\\n",
    ".addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立一個 5-fold CrossValidator\n",
    "cv=CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-f5c222053638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvModel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cvModel=cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train=cvModel.transform(train)\n",
    "predict_test=cvModel.transform(test)\n",
    "print(\"訓練資料集之ROC，經過CV後:{}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"測試資料集之ROC，經過CV後:{}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
