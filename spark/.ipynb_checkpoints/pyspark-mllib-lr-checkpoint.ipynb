{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://be272d7ce1c9:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-mllib</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-mllib>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"pyspark-mllib-lr\").getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binary classification in pySpark\n",
    "* 共有九個變數 (八個自變數，一個應變數)\n",
    "* Pregnancies: 懷孕次數\n",
    "* Glucose: 葡萄糖\n",
    "* BloodPressure:血壓\n",
    "* SkinThickness: 皮膚厚度 \n",
    "* Insulin: 胰島素\n",
    "* BMI: 來衡量肥胖程度，其計算公式是以體重（公斤）除以身高（公尺）的平方\n",
    "* DiabetesPedigreeFunction: 糖尿病家族函數\n",
    "* Age: 年紀 (歲)\n",
    "* Outcome: 類別結果 (0或1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'Insulin',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age',\n",
       " 'Outcome']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data\n",
    "raw_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(r\"/home/jovyan/dataset/diabetes.csv\")\n",
    "raw_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|Summary|       Pregnancies|          Glucose|     BloodPressure|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|               768|              768|               768|\n",
      "|   mean|3.8450520833333335|     120.89453125|       69.10546875|\n",
      "| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|\n",
      "|    min|                 0|                0|                 0|\n",
      "|    max|                17|              199|               122|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"Pregnancies\",\"Glucose\",\"BloodPressure\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|Summary|     SkinThickness|           Insulin|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               768|               768|\n",
      "|   mean|20.536458333333332| 79.79947916666667|\n",
      "| stddev|15.952217567727642|115.24400235133803|\n",
      "|    min|                 0|                 0|\n",
      "|    max|                99|               846|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"SkinThickness\",\"Insulin\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------------+------------------+\n",
      "|Summary|               BMI|DiabetesPedigreeFunction|               Age|\n",
      "+-------+------------------+------------------------+------------------+\n",
      "|  count|               768|                     768|               768|\n",
      "|   mean|31.992578124999977|      0.4718763020833327|33.240885416666664|\n",
      "| stddev| 7.884160320375441|       0.331328595012775|11.760231540678689|\n",
      "|    min|               0.0|                   0.078|                21|\n",
      "|    max|              67.1|                    2.42|                81|\n",
      "+-------+------------------+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 發現有一些欄位的最小值為 0 \n",
    "## 除了 Pregnancies 之外，其他的不切實際\n",
    "* Pregnancies\n",
    "* Glucose\n",
    "* BloodPressure\n",
    "* SkinThickness\n",
    "* Insulin\n",
    "* BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將 min為0的值，改為 Nan\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"Glucose\",when(raw_data.Glucose==0, np.nan).otherwise(raw_data.Glucose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"BloodPressure\",when(raw_data.BloodPressure==0, np.nan).otherwise(raw_data.BloodPressure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"SkinThickness\",when(raw_data.SkinThickness==0, np.nan).otherwise(raw_data.SkinThickness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"Insulin\",when(raw_data.Insulin==0, np.nan).otherwise(raw_data.Insulin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.withColumn(\"BMI\",when(raw_data.BMI==0, np.nan).otherwise(raw_data.BMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-------------+----+-------+\n",
      "|Summary|Glucose|BloodPressure|SkinThickness| BMI|Insulin|\n",
      "+-------+-------+-------------+-------------+----+-------+\n",
      "|  count|    768|          768|          768| 768|    768|\n",
      "|   mean|    NaN|          NaN|          NaN| NaN|    NaN|\n",
      "| stddev|    NaN|          NaN|          NaN| NaN|    NaN|\n",
      "|    min|   44.0|         24.0|          7.0|18.2|   14.0|\n",
      "|    max|    NaN|          NaN|          NaN| NaN|    NaN|\n",
      "+-------+-------+-------------+-------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#預設有三種方式填充遺缺值 (平均值(mean)、中位數(median))\n",
    "imputer=Imputer(inputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"],\n",
    "                outputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"]).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|  148.0|         72.0|         35.0|  125.0|33.6|                   0.627| 50|      1|\n",
      "|          1|   85.0|         66.0|         29.0|  125.0|26.6|                   0.351| 31|      0|\n",
      "|          8|  183.0|         64.0|         29.0|  125.0|23.3|                   0.672| 32|      1|\n",
      "|          1|   89.0|         66.0|         23.0|   94.0|28.1|                   0.167| 21|      0|\n",
      "|          0|  137.0|         40.0|         35.0|  168.0|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=imputer.fit(raw_data)\n",
    "raw_data=model.transform(raw_data)\n",
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=raw_data.columns\n",
    "cols.remove(\"Outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將資料轉成一維向量\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|features                                    |\n",
      "+--------------------------------------------+\n",
      "|[6.0,148.0,72.0,35.0,125.0,33.6,0.627,50.0] |\n",
      "|[1.0,85.0,66.0,29.0,125.0,26.6,0.351,31.0]  |\n",
      "|[8.0,183.0,64.0,29.0,125.0,23.3,0.672,32.0] |\n",
      "|[1.0,89.0,66.0,23.0,94.0,28.1,0.167,21.0]   |\n",
      "|[0.0,137.0,40.0,35.0,168.0,43.1,2.288,33.0] |\n",
      "|[5.0,116.0,74.0,29.0,125.0,25.6,0.201,30.0] |\n",
      "|[3.0,78.0,50.0,32.0,88.0,31.0,0.248,26.0]   |\n",
      "|[10.0,115.0,72.0,29.0,125.0,35.3,0.134,29.0]|\n",
      "|[2.0,197.0,70.0,45.0,543.0,30.5,0.158,53.0] |\n",
      "|[8.0,125.0,96.0,29.0,125.0,32.3,0.232,54.0] |\n",
      "|[4.0,110.0,92.0,29.0,125.0,37.6,0.191,30.0] |\n",
      "|[10.0,168.0,74.0,29.0,125.0,38.0,0.537,34.0]|\n",
      "|[10.0,139.0,80.0,29.0,125.0,27.1,1.441,57.0]|\n",
      "|[1.0,189.0,60.0,23.0,846.0,30.1,0.398,59.0] |\n",
      "|[5.0,166.0,72.0,19.0,175.0,25.8,0.587,51.0] |\n",
      "|[7.0,100.0,72.0,29.0,125.0,30.0,0.484,32.0] |\n",
      "|[0.0,118.0,84.0,47.0,230.0,45.8,0.551,31.0] |\n",
      "|[7.0,107.0,74.0,29.0,125.0,29.6,0.254,31.0] |\n",
      "|[1.0,103.0,30.0,38.0,83.0,43.3,0.183,33.0]  |\n",
      "|[1.0,115.0,70.0,30.0,96.0,34.6,0.529,32.0]  |\n",
      "+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data=assembler.transform(raw_data)\n",
    "raw_data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard scalarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=standardscaler.fit(raw_data).transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     Scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[6.0,148.0,72.0,3...|[1.78063837321943...|\n",
      "|[1.0,85.0,66.0,29...|[0.29677306220323...|\n",
      "|[8.0,183.0,64.0,2...|[2.37418449762590...|\n",
      "|[1.0,89.0,66.0,23...|[0.29677306220323...|\n",
      "|[0.0,137.0,40.0,3...|[0.0,4.5009104914...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 建立一個新欄位，將既有的 features 欄位 Standard scalarizer 後，產生一個新的欄位 Scaled_features\n",
    "raw_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立訓練集(80%)、測試集(20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = raw_data.randomSplit([0.8,0.2], seed=123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 檢查是否有imbalance的問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "為1: 221\n",
      "為0: 402.0\n",
      "為1的百分比: 35.47351524879615\n"
     ]
    }
   ],
   "source": [
    "#檢查是否有imbalance的問題\n",
    "dataset_size=float(train.select(\"Outcome\").count())\n",
    "numPositives=train.select(\"Outcome\").where(\"Outcome==1\").count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('為1: {}'.format(numPositives))\n",
    "print('為0: {}'.format(numNegatives))\n",
    "print('為1的百分比: {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalancingRatio=0.6452648475120385\n"
     ]
    }
   ],
   "source": [
    "# 的確有 imbalance的問題，為1的比例為35.47%，因此我們可以將為1的資料給予一個較大的權重\n",
    "# BalancingRatio=numNegatives/dataset_size\n",
    "\n",
    "BalancingRation=numNegatives/dataset_size\n",
    "print('BalancingRatio={}'.format(BalancingRation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      classWeights|\n",
      "+------------------+\n",
      "|0.3547351524879615|\n",
      "|0.3547351524879615|\n",
      "|0.3547351524879615|\n",
      "|0.3547351524879615|\n",
      "|0.3547351524879615|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#為1: 我們放入classWeights，內容為BalancingRatio\n",
    "#為0: 我們放入classWeights，內容為1-BalancingRatio\n",
    "train=train.withColumn(\"classWeights\",when(train.Outcome==1, BalancingRation).otherwise(1-BalancingRation))\n",
    "train.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection (採用 ChiSqSelector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark MLlib 具有三種方法找特徵值 (VectorSlicer/RFormula/ChiSqSelector:(Chi-Squared feature selection))\n",
    "#Chi-Squared feature selection: 適用於目標值為類別型變數\n",
    "\n",
    "from pyspark.ml.feature import ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fpr chooses all features whose p-values are below a threshold, thus controlling the false positive rate of selection.\n",
    "css=ChiSqSelector(featuresCol='Scaled_features',outputCol='Aspect',labelCol='Outcome',fpr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=css.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=css.fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Aspect                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.0,2.2011752038500245,6.282735462564321,3.2987454102033076,1.4470429786115504,6.588921448662343,0.5855214518762558,3.911487613223074]|\n",
      "|[0.0,2.7596823451254036,5.290724600054165,2.502496518085268,0.7640386927068986,5.207138804903132,1.6448927385183476,1.785679127775751] |\n",
      "|[0.0,2.989655873885854,5.62139488755755,3.639994935396753,2.431032204067405,5.803487103788687,1.1499158410559458,2.125808485447323]    |\n",
      "|[0.0,3.1210693188918257,6.613405750067706,5.118742877901684,1.065023632258101,5.3089543681274955,0.995990098552394,2.210840824865216]  |\n",
      "|[0.0,3.252482763897797,5.952065175060935,3.2987454102033076,1.4470429786115504,3.6362701151558188,0.7635924088901687,1.870711467193644]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.select(\"Aspect\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立羅吉斯回歸分類演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"Aspect\", weightCol=\"classWeights\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       1.0|\n",
      "|      1|       0.0|\n",
      "|      1|       0.0|\n",
      "|      0|       0.0|\n",
      "|      1|       1.0|\n",
      "|      1|       1.0|\n",
      "|      0|       0.0|\n",
      "|      0|       1.0|\n",
      "|      1|       1.0|\n",
      "|      0|       1.0|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.select(\"Outcome\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型評估\n",
    "*預設用ROC評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"Outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+\n",
      "|Outcome|       rawPrediction|prediction|         probability|\n",
      "+-------+--------------------+----------+--------------------+\n",
      "|      0|[1.93251831726415...|       0.0|[0.87352789759305...|\n",
      "|      0|[1.58456515697427...|       0.0|[0.82985008420462...|\n",
      "|      0|[1.49187237243758...|       0.0|[0.81635913912982...|\n",
      "|      0|[1.88417237871225...|       0.0|[0.86808963982654...|\n",
      "|      0|[2.87706309674848...|       0.0|[0.94670086661773...|\n",
      "+-------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.select(\"Outcome\",\"rawPrediction\",\"prediction\",\"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for training set is 0.8351005155219385\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for training set is {}\".format(evaluator.evaluate(predict_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for testing set is 0.8712548849326966\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for testing set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超參數\n",
    "* 模型優化(model tuning)\n",
    "\n",
    "* aggregationDepth: 調整層次深度，該值越大，表示能更快處理資料(>=2的整數，預設為2)\n",
    "* elasticNetParam: 正規化調整計算參數(0~1之間，預設為：0)\n",
    "* family: 採用分類方式 (二元分類)\n",
    "* featuresCol: 輸入訓練集、測試集之變數向量(預設為features)\n",
    "* fitIntercept: 是否使用帶截距的回歸。(預設為true)\n",
    "* labelCol: 目標欄位(預設為label)\n",
    "* maxIter: 迭代算法最大迭代次數，搭配tol值一起設定的結束條件(預設為100)\n",
    "* tol: 收斂容忍係數(convergence tolerance)，算法每次迭代後的比較阈值以確定算法是否結束，值越小，执行的迭代次數就越多(預設值1.0E-6)\n",
    "* predictionCol: 輸出結果資料中最終判別類別名稱(預設值為prediction)\n",
    "* probabilityCol: 輸出結果資料之Softmax函數機率值的名稱(預設為probability)\n",
    "* rawPredictionCol: 輸出結果資料之回歸應變數名稱(預設為rawPrediction)\n",
    "* regParam: 正規化懲罰程度參數(預設為0) \n",
    "* standardization: 是否在回歸前對自變數進行標準化處理(預設為True)\n",
    "* threshold: 二元分類演算法之中根據該值將計算得到之邏輯函數機率值對應於分類類別中(預設為5)\n",
    "* thresholds: 多元分類演算法中根據該組數值將計算後得到的Softmax函數機率對應於多元分類中(無預設值)\n",
    "* weightCol: 輸入訓練集權重的名稱，如果輸入的訓練集包含權重，則採用帶權重的回歸模型，如果不包含，相當於全重是1.0 (無預設值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#設計一個超參數grid\n",
    "paramGrid=ParamGridBuilder() \\\n",
    ".addGrid(lr.aggregationDepth,[2,5,10]) \\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    ".addGrid(lr.fitIntercept,[False, True]) \\\n",
    ".addGrid(lr.maxIter,[10,100,1000]) \\\n",
    ".addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立一個 5-fold CrossValidator\n",
    "cv=CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f3d85cc11e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'LogisticRegression' object has no attribute '_java_obj'\n",
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f3d85cc11e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'LogisticRegression' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "cvModel=cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train=cvModel.transform(train)\n",
    "predict_test=cvModel.transform(test)\n",
    "print(\"訓練資料集之ROC，經過CV後:{}\".format(evaluate(predict_train)))\n",
    "print(\"測試資料集之ROC，經過CV後:{}\".format(evaluate(predict_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
